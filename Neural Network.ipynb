{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self, l1Size, l2Size):\n",
    "        self.l1Size = l1Size\n",
    "        self.l2Size = l2Size\n",
    "        \n",
    "    def randInit(self, x, y):\n",
    "        n = x.shape[1] #features\n",
    "        classes = y.shape[1] #classes\n",
    "        self.l1W = np.random.rand(n, self.l1Size) - 0.5\n",
    "        self.l2W = np.random.rand(self.l1Size + 1, self.l2Size) - 0.5\n",
    "        self.oW = np.random.rand(self.l2Size + 1, classes) - 0.5\n",
    "\n",
    "        \n",
    "    def forwardProp(self, x):\n",
    "        def addBias(x):\n",
    "            return np.append(np.ones((x.shape[0], 1)), x, axis=1)\n",
    "        \n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        z1 = x @ self.l1W\n",
    "        a1 = z1 * (z1 > 0)\n",
    "        z2 = addBias(a1) @ self.l2W\n",
    "        a2 = z2 * (z2 > 0)\n",
    "        z3 = addBias(a2) @ self.oW\n",
    "        a3 = np.minimum(0.9999, np.maximum(0.0001, sigmoid(z3)))\n",
    "        return (a3, a2, a1);\n",
    "        \n",
    "    def costFunc(self, x, y):   \n",
    "        a3 = self.forwardProp(x)[0]\n",
    "        return sum(sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))) / -y.shape[0]          \n",
    "        \n",
    "    def optFunc(self, x, y, algo = \"gradientdescent\", \n",
    "                alpha = 0.01, beta = 0.9, beta2 = 0.999, \n",
    "                batchSize = 128):\n",
    "        def addBias(x):\n",
    "            return np.append(np.ones((x.shape[0], 1)), x, axis=1)\n",
    "        \n",
    "        def getDerivative():\n",
    "            a3, a2, a1 = self.forwardProp(x)\n",
    "            \n",
    "            #Backprop\n",
    "            a3Err = (a3 - y) / x.shape[0]\n",
    "            a2Err = (a3Err @ self.oW.T)\n",
    "            a1Err = (a2Err[:,1:] @ self.l2W.T)\n",
    "\n",
    "            oD = addBias(a2).T @ a3Err \n",
    "            l2D = addBias(a1).T @ a2Err[:,1:]  \n",
    "            l1D = x.T @ a1Err[:,1:]\n",
    "            return (l1D, l2D, oD)\n",
    "        \n",
    "        def gradientDescent():\n",
    "            def optimize():\n",
    "                ders = getDerivative()\n",
    "                self.l1W -= alpha * ders[0]\n",
    "                self.l2W -= alpha * ders[1]\n",
    "                self.oW -= alpha * ders[2]\n",
    "            return optimize\n",
    "        \n",
    "        def GDWMomentum():\n",
    "            dersO = getDerivative()\n",
    "            l1WB = dersO[0]\n",
    "            l2WB = dersO[1]\n",
    "            oWB = dersO[2]\n",
    "            def optimize():\n",
    "                nonlocal l1WB\n",
    "                nonlocal l2WB\n",
    "                nonlocal oWB\n",
    "                ders = getDerivative()\n",
    "                l1WB = beta * l1WB + (1 - beta) * ders[0]\n",
    "                l2WB = beta * l2WB + (1 - beta) * ders[1]\n",
    "                oWB = beta * oWB + (1 - beta) * ders[2]\n",
    "                self.l1W -= alpha * l1WB\n",
    "                self.l2W -= alpha * l2WB\n",
    "                self.oW -= alpha * oWB\n",
    "            return optimize\n",
    "        \n",
    "        def RMSProp():\n",
    "            dersO = getDerivative()\n",
    "            l1WS = dersO[0] * dersO[0]\n",
    "            l2WS = dersO[1] * dersO[1]\n",
    "            oWS = dersO[2] * dersO[2]\n",
    "            epsilon = 0.0000001\n",
    "            def optimize():\n",
    "                nonlocal l1WS\n",
    "                nonlocal l2WS\n",
    "                nonlocal oWS\n",
    "                ders = getDerivative()\n",
    "                l1WS = beta * l1WS + (1 - beta) * ders[0] * ders[0]\n",
    "                l2WS = beta * l2WS + (1 - beta) * ders[1] * ders[1]\n",
    "                oWS = beta * oWS + (1 - beta) * ders[2] * ders[2]\n",
    "                self.l1W -= alpha * ders[0] / (np.sqrt(l1WS) + epsilon)\n",
    "                self.l2W -= alpha * ders[1] / (np.sqrt(l2WS) + epsilon)\n",
    "                self.oW -= alpha * ders[2] / (np.sqrt(oWS) + epsilon)\n",
    "            return optimize\n",
    "        \n",
    "        def ADAM():\n",
    "            dersO = getDerivative()\n",
    "            l1WB = dersO[0]\n",
    "            l2WB = dersO[1]\n",
    "            oWB = dersO[2]\n",
    "            l1WS = dersO[0] * dersO[0]\n",
    "            l2WS = dersO[1] * dersO[1]\n",
    "            oWS = dersO[2] * dersO[2]\n",
    "            epsilon = 0.0000001\n",
    "            def optimize():\n",
    "                nonlocal l1WB\n",
    "                nonlocal l2WB\n",
    "                nonlocal oWB\n",
    "                nonlocal l1WS\n",
    "                nonlocal l2WS\n",
    "                nonlocal oWS\n",
    "                ders = getDerivative()\n",
    "                l1WB = beta * l1WB + (1 - beta) * ders[0]\n",
    "                l2WB = beta * l2WB + (1 - beta) * ders[1]\n",
    "                oWB = beta * oWB + (1 - beta) * ders[2]\n",
    "                l1WS = beta * l1WS + (1 - beta) * ders[0] * ders[0]\n",
    "                l2WS = beta * l2WS + (1 - beta) * ders[1] * ders[1]\n",
    "                oWS = beta * oWS + (1 - beta) * ders[2] * ders[2]\n",
    "                self.l1W -= alpha * l1WB / (np.sqrt(l1WS) + epsilon)\n",
    "                self.l2W -= alpha * l2WB / (np.sqrt(l2WS) + epsilon)\n",
    "                self.oW -= alpha * oWB / (np.sqrt(oWS) + epsilon)\n",
    "            return optimize\n",
    "            \n",
    "        def switch(x):\n",
    "            return {\n",
    "                \"gradientdescent\": gradientDescent(),\n",
    "                \"gd\": gradientDescent(),\n",
    "                \"gdwmomentum\": GDWMomentum(),\n",
    "                \"momentum\": GDWMomentum(),\n",
    "                \"rmsprop\": RMSProp(),\n",
    "                \"adam\": ADAM()\n",
    "            }.get(x.replace(\" \", \"\").lower(), gradientDescent) \n",
    "        return switch(algo)\n",
    "    \n",
    "    def train(self, x, y, iters = 500):\n",
    "        optimize = self.optFunc(x, y, \"adam\", 0.002)\n",
    "        for i in range(1,iters):\n",
    "            optimize()\n",
    "            if(i % 20 == 0):\n",
    "                print(\"Iter: \", i, \" JVal: \", self.costFunc(x, y))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.forwardProp(x)[0]\n",
    "            \n",
    "            \n",
    "def testAccuracy(predict, label):\n",
    "    n = label.shape[1]\n",
    "    m = label.shape[0]\n",
    "    maxes = np.tile(np.array([np.max(predict, 1)]).T, (1, n))\n",
    "    oneHot = np.array(maxes) == np.array(predict)\n",
    "    return (sum(sum(oneHot == label)) / (m) - (n-2)) / 2\n",
    "\n",
    "        \n",
    "n = neuralNetwork(100, 100)\n",
    "trainX = mnist.train.images[0:10000] \n",
    "trainY = mnist.train.labels[0:10000]\n",
    "n.randInit(trainX, trainY)\n",
    "n.train(trainX, trainY, 500)\n",
    "\n",
    "testX = mnist.test.images[0:2000]\n",
    "testY = mnist.test.labels[0:2000]\n",
    "\n",
    "print(testAccuracy(n.predict(testX), testY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#testing code\n",
    "     def checkDerivative(self, x, y):\n",
    "        oWDer = np.zeros((self.oW.shape[0], self.oW.shape[1]))\n",
    "        for i in range(self.oW.shape[0]):\n",
    "            for j in range(self.oW.shape[1]):\n",
    "                self.oW[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.oW[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                oWDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.oW[i][j] += 0.00001\n",
    "                \n",
    "        l2WDer = np.ones((self.l2W.shape[0], self.l2W.shape[1]))\n",
    "        for i in range(self.l2W.shape[0]):\n",
    "            for j in range(self.l2W.shape[1]):\n",
    "                self.l2W[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.l2W[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l2WDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.l2W[i][j] += 0.00001\n",
    "                \n",
    "        l1WDer = np.ones((self.l1W.shape[0], self.l1W.shape[1]))\n",
    "        for i in range(self.l1W.shape[0]):\n",
    "            for j in range(self.l1W.shape[1]):\n",
    "                self.l1W[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.l1W[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l1WDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.l1W[i][j] += 0.00001\n",
    "                \n",
    "        print(\"Done\")\n",
    "        return (l1WDer, l2WDer,oWDer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#if you really want to implement batch norm\n",
    "#https://chrisyeh96.github.io/2017/08/28/deriving-batchnorm-backprop.html\n",
    "\n",
    "class neuralNetwork:\n",
    "    def __init__(self, l1Size, l2Size):\n",
    "        self.l1Size = l1Size\n",
    "        self.l2Size = l2Size\n",
    "        \n",
    "    def randInit(self, x, y):\n",
    "        n = x.shape[1] #features\n",
    "        classes = y.shape[1] #classes\n",
    "        self.l1W = np.random.rand(n, self.l1Size) - 0.5\n",
    "        self.l2W = np.random.rand(self.l1Size + 1, self.l2Size) - 0.5\n",
    "        self.oW = np.random.rand(self.l2Size + 1, classes) - 0.5\n",
    "        self.gamma2 = np.ones(self.l1Size + 1)\n",
    "        self.beta2 = np.zeros(self.l1Szie + 1)\n",
    "\n",
    "        \n",
    "    def forwardProp(self, x):\n",
    "        def addBias(x):\n",
    "            return np.append(np.ones((x.shape[0], 1)), x, axis=1)\n",
    "        \n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        z1 = x @ self.l1W\n",
    "        a1 = z1 * (z1 > 0)\n",
    "        u1 = np.mean(a1)\n",
    "        s1 = np.std(a1)\n",
    "        zNorm2 = (a1-u1)/s1\n",
    "        z2 = addBias(self.gamma2 * zNorm2 + self.beta2) @ self.l2W\n",
    "        a2 = z2 * (z2 > 0)\n",
    "        z3 = addBias(a2) @ self.oW\n",
    "        a3 = np.minimum(0.9999, np.maximum(0.0001, sigmoid(z3)))\n",
    "        return (a3, a2, a1, Zorm2)\n",
    "    \n",
    "    def costFunc(self, x, y):   \n",
    "        a3 = self.forwardProp(x)[0]\n",
    "        return sum(sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))) / -y.shape[0]          \n",
    "        \n",
    "    def optFunc(self, x, y, algo = \"gradientdescent\", \n",
    "                alpha = 0.01, beta = 0.9, beta2 = 0.999, \n",
    "                batchSize = 128):\n",
    "        def addBias(x):\n",
    "            return np.append(np.ones((x.shape[0], 1)), x, axis=1)\n",
    "        \n",
    "        def getDerivative():\n",
    "            a3, a2, a1, zNorm2 = self.forwardProp(x)\n",
    "            \n",
    "            #Backprop\n",
    "            a3Err = (a3 - y) / x.shape[0]\n",
    "            a2Err = (a3Err @ self.oW.T)\n",
    "            a1Err = (a2Err[:,1:] @ self.l2W.T)\n",
    "\n",
    "            oD = addBias(a2).T @ a3Err \n",
    "            l2D = addBias(a1).T @ a2Err[:,1:]\n",
    "            gamma2D = np.(a2Err * zNorm2)\n",
    "            beta2D = np.mean(a2Err)\n",
    "            #\n",
    "            # Todo is here\n",
    "            #\n",
    "            ###l1D needs to be changed\n",
    "            l1D = x.T @ a1Err[:,1:]\n",
    "            return (l1D, l2D, oD)\n",
    "        \n",
    "        def gradientDescent():\n",
    "            def optimize():\n",
    "                ders = getDerivative()\n",
    "                self.l1W -= alpha * ders[0]\n",
    "                self.l2W -= alpha * ders[1]\n",
    "                self.oW -= alpha * ders[2]\n",
    "            return optimize\n",
    "        \n",
    "        def GDWMomentum():\n",
    "            dersO = getDerivative()\n",
    "            l1WB = dersO[0]\n",
    "            l2WB = dersO[1]\n",
    "            oWB = dersO[2]\n",
    "            def optimize():\n",
    "                nonlocal l1WB\n",
    "                nonlocal l2WB\n",
    "                nonlocal oWB\n",
    "                ders = getDerivative()\n",
    "                l1WB = beta * l1WB + (1 - beta) * ders[0]\n",
    "                l2WB = beta * l2WB + (1 - beta) * ders[1]\n",
    "                oWB = beta * oWB + (1 - beta) * ders[2]\n",
    "                self.l1W -= alpha * l1WB\n",
    "                self.l2W -= alpha * l2WB\n",
    "                self.oW -= alpha * oWB\n",
    "            return optimize\n",
    "        \n",
    "        def RMSProp():\n",
    "            dersO = getDerivative()\n",
    "            l1WS = dersO[0] * dersO[0]\n",
    "            l2WS = dersO[1] * dersO[1]\n",
    "            oWS = dersO[2] * dersO[2]\n",
    "            epsilon = 0.0000001\n",
    "            def optimize():\n",
    "                nonlocal l1WS\n",
    "                nonlocal l2WS\n",
    "                nonlocal oWS\n",
    "                ders = getDerivative()\n",
    "                l1WS = beta * l1WS + (1 - beta) * ders[0] * ders[0]\n",
    "                l2WS = beta * l2WS + (1 - beta) * ders[1] * ders[1]\n",
    "                oWS = beta * oWS + (1 - beta) * ders[2] * ders[2]\n",
    "                self.l1W -= alpha * ders[0] / (np.sqrt(l1WS) + epsilon)\n",
    "                self.l2W -= alpha * ders[1] / (np.sqrt(l2WS) + epsilon)\n",
    "                self.oW -= alpha * ders[2] / (np.sqrt(oWS) + epsilon)\n",
    "            return optimize\n",
    "        \n",
    "        def ADAM():\n",
    "            dersO = getDerivative()\n",
    "            l1WB = dersO[0]\n",
    "            l2WB = dersO[1]\n",
    "            oWB = dersO[2]\n",
    "            l1WS = dersO[0] * dersO[0]\n",
    "            l2WS = dersO[1] * dersO[1]\n",
    "            oWS = dersO[2] * dersO[2]\n",
    "            epsilon = 0.0000001\n",
    "            def optimize():\n",
    "                nonlocal l1WB\n",
    "                nonlocal l2WB\n",
    "                nonlocal oWB\n",
    "                nonlocal l1WS\n",
    "                nonlocal l2WS\n",
    "                nonlocal oWS\n",
    "                ders = getDerivative()\n",
    "                l1WB = beta * l1WB + (1 - beta) * ders[0]\n",
    "                l2WB = beta * l2WB + (1 - beta) * ders[1]\n",
    "                oWB = beta * oWB + (1 - beta) * ders[2]\n",
    "                l1WS = beta * l1WS + (1 - beta) * ders[0] * ders[0]\n",
    "                l2WS = beta * l2WS + (1 - beta) * ders[1] * ders[1]\n",
    "                oWS = beta * oWS + (1 - beta) * ders[2] * ders[2]\n",
    "                self.l1W -= alpha * l1WB / (np.sqrt(l1WS) + epsilon)\n",
    "                self.l2W -= alpha * l2WB / (np.sqrt(l2WS) + epsilon)\n",
    "                self.oW -= alpha * oWB / (np.sqrt(oWS) + epsilon)\n",
    "            return optimize\n",
    "            \n",
    "        def switch(x):\n",
    "            return {\n",
    "                \"gradientdescent\": gradientDescent(),\n",
    "                \"gd\": gradientDescent(),\n",
    "                \"gdwmomentum\": GDWMomentum(),\n",
    "                \"momentum\": GDWMomentum(),\n",
    "                \"rmsprop\": RMSProp(),\n",
    "                \"adam\": ADAM()\n",
    "            }.get(x.replace(\" \", \"\").lower(), gradientDescent) \n",
    "        return switch(algo)\n",
    "    \n",
    "    def train(self, x, y, iters = 500):\n",
    "        optimize = self.optFunc(x, y, \"adam\", 0.002)\n",
    "        for i in range(1,iters):\n",
    "            optimize()\n",
    "            if(i % 20 == 0):\n",
    "                print(\"Iter: \", i, \" JVal: \", self.costFunc(x, y))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.forwardProp(x)[0]\n",
    "            \n",
    "    def checkDerivative(self, x, y):\n",
    "        oWDer = np.zeros((self.oW.shape[0], self.oW.shape[1]))\n",
    "        for i in range(self.oW.shape[0]):\n",
    "            for j in range(self.oW.shape[1]):\n",
    "                self.oW[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.oW[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                oWDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.oW[i][j] += 0.00001\n",
    "                \n",
    "        l2WDer = np.ones((self.l2W.shape[0], self.l2W.shape[1]))\n",
    "        for i in range(self.l2W.shape[0]):\n",
    "            for j in range(self.l2W.shape[1]):\n",
    "                self.l2W[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.l2W[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l2WDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.l2W[i][j] += 0.00001\n",
    "                \n",
    "        l1WDer = np.ones((self.l1W.shape[0], self.l1W.shape[1]))\n",
    "        for i in range(self.l1W.shape[0]):\n",
    "            for j in range(self.l1W.shape[1]):\n",
    "                self.l1W[i][j] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.l1W[i][j] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l1WDer[i][j] = (upper - lower) / 0.00002\n",
    "                self.l1W[i][j] += 0.00001\n",
    "                \n",
    "        gamma2Der = np.ones(self.l1Size + 1)\n",
    "            for i in range(self.l1Size + 1):\n",
    "                self.gamma2[i] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.gamma2[i] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l1WDer[i] = (upper - lower) / 0.00002\n",
    "                self.gamma2[i] += 0.00001\n",
    "        \n",
    "        beta2Der = np.ones(self.l1Size + 1)\n",
    "            for i in range(self.l1Size + 1):\n",
    "                self.l1W[i] += 0.00001\n",
    "                upper = self.costFunc(x, y)\n",
    "                self.beta2[i] -= 0.00002\n",
    "                lower = self.costFunc(x, y)\n",
    "                l1WDer[i] = (upper - lower) / 0.00002\n",
    "                self.beta2[i] += 0.00001\n",
    "            \n",
    "        return (l1WDer, l2WDer,oWDer)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
